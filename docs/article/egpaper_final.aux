\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{batchnorm}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\citation{crelu}
\citation{drelu}
\citation{crelu}
\citation{drelu}
\citation{leakyrelu}
\citation{prelu}
\citation{selu}
\citation{batchnorm}
\citation{swish}
\citation{relu_neurology}
\citation{firstrelu}
\citation{alexnet}
\citation{leakyrelu}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Background and state-of-the-art}{2}{section.2}}
\newlabel{sec:review}{{2}{2}{\hskip -1em.~Background and state-of-the-art}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Rectified Linear Unit}{2}{subsection.2.1}}
\newlabel{eq:relu}{{1}{2}{\hskip -1em.~Rectified Linear Unit}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Leaky Rectified Linear Units}{2}{subsection.2.2}}
\citation{prelu}
\citation{selu}
\citation{ELU}
\citation{swish}
\citation{batchnorm}
\newlabel{eq:leakyrelu}{{2}{3}{\hskip -1em.~Leaky Rectified Linear Units}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.\nobreakspace  {}Parametric ReLU}{3}{subsection.2.3}}
\newlabel{eq:parametricrelu}{{3}{3}{\hskip -1em.~Parametric ReLU}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}\hskip -1em.\nobreakspace  {}Scaled Exponential Rectified Unit}{3}{subsection.2.4}}
\newlabel{eq:selu}{{4}{3}{\hskip -1em.~Scaled Exponential Rectified Unit}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}\hskip -1em.\nobreakspace  {}Swish}{3}{subsection.2.5}}
\newlabel{eq:swish}{{5}{3}{\hskip -1em.~Swish}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}\hskip -1em.\nobreakspace  {}Batch Normalization}{3}{subsection.2.6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Linked neurons}{3}{section.3}}
\newlabel{sec:proposal}{{3}{3}{\hskip -1em.~Linked neurons}{section.3}{}}
\citation{crelu}
\citation{allcnn}
\citation{resnet}
\citation{chollet2015keras}
\citation{tensorflow2015-whitepaper}
\citation{code}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Two different examples of how to link two neurons: (a) shows two ReLUs coupled, (b) shows another option for coupling using SELU as base activation.}}{4}{figure.1}}
\newlabel{fig:coupling}{{1}{4}{Two different examples of how to link two neurons: (a) shows two ReLUs coupled, (b) shows another option for coupling using SELU as base activation}{figure.1}{}}
\newlabel{eq:doublerelu}{{6}{4}{\hskip -1em.~Linked neurons}{equation.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Experiments and results.}{4}{section.4}}
\newlabel{sec:experiments}{{4}{4}{\hskip -1em.~Experiments and results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Effect of linked neurons on wide and deep architectures.}{4}{subsection.4.1}}
\citation{adam}
\citation{rmsprop}
\citation{allcnn}
\citation{resnet}
\citation{allcnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Number of parameters}{5}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.\nobreakspace  {}Performance-focused experiments}{5}{subsection.4.3}}
\citation{resnet}
\citation{adam}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Width experiment.}}{6}{table.1}}
\newlabel{tab:onelayer}{{1}{6}{Width experiment}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Depth experiment.}}{6}{table.2}}
\newlabel{tab:basicnet}{{2}{6}{Depth experiment}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Number of parameters accuracy comparison.}}{6}{table.3}}
\newlabel{tab:doubles}{{3}{6}{Number of parameters accuracy comparison}{table.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Conclusion, advantages and future work}{6}{section.5}}
\newlabel{sec:conclusions}{{5}{6}{\hskip -1em.~Conclusion, advantages and future work}{section.5}{}}
\bibstyle{ieee}
\bibdata{egbib}
\bibcite{tensorflow2015-whitepaper}{1}
\bibcite{chollet2015keras}{2}
\bibcite{ELU}{3}
\bibcite{firstrelu}{4}
\bibcite{drelu}{5}
\bibcite{relu_neurology}{6}
\bibcite{prelu}{7}
\bibcite{resnet}{8}
\bibcite{batchnorm}{9}
\bibcite{adam}{10}
\bibcite{selu}{11}
\bibcite{alexnet}{12}
\bibcite{leakyrelu}{13}
\bibcite{swish}{14}
\bibcite{code}{15}
\bibcite{crelu}{16}
\bibcite{allcnn}{17}
\bibcite{rmsprop}{18}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Allcnn experiment.}}{7}{table.4}}
\newlabel{tab:allcnn}{{4}{7}{Allcnn experiment}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces ResNet50 experiment.}}{7}{table.5}}
\newlabel{tab:resnet50}{{5}{7}{ResNet50 experiment}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces ResNet50 without Batch Normlization experiment.}}{7}{table.6}}
\newlabel{tab:resnet50_nobatchnorm}{{6}{7}{ResNet50 without Batch Normlization experiment}{table.6}{}}
